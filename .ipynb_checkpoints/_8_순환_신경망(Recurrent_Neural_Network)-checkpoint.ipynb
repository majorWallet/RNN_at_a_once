{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jacLnynuqUXZ"
   },
   "source": [
    "# 순환 신경망 (Recurrent Neural Network, RNN)\n",
    "\n",
    "- **순서가 있는 데이터**를 입력으로 받음\n",
    "\n",
    "- 변화하는 입력에 대한 출력을 얻음\n",
    "\n",
    "- 시계열(날씨, 주가 등), 자연어와 같이 **시간의 흐름에 따라 변화하고, 그 변화가 의미를 갖는 데이터** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3P0gpo4gqWUz"
   },
   "source": [
    "## Feed Forward Network vs Recurrent Network\n",
    "\n",
    "- Feed Forward Net (앞먹임 구조)\n",
    "  - 일반적인 구조의 신경망\n",
    "\n",
    "  - 입력 → 은닉 → 출력층 으로 이어지는 단방향 구조\n",
    "\n",
    "  - 이전 스텝의 출력의 영향을 받지 않음\n",
    "\n",
    "- Recurrent Net (되먹임 구조)\n",
    "  - 이전 층(Layer), 또는 스텝의 출력이 다시 입력으로 연결되는 신경망 구조\n",
    "\n",
    "  - 각 스텝마다 이전 상태를 기억 시스템(Memory System)  \n",
    "\n",
    "  - 현재 상태가 이전 상태에 종속\n",
    "\n",
    "  <br>\n",
    "\n",
    "  <img src=\"https://www.researchgate.net/profile/Engin_Pekel/publication/315111480/figure/fig1/AS:472548166115333@1489675670530/Feed-forward-and-recurrent-ANN-architecture.png\">\n",
    "\n",
    "  <sub>출처: https://www.researchgate.net/figure/Feed-forward-and-recurrent-ANN-architecture_fig1_315111480</sub>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2h5HFH0BqYho"
   },
   "source": [
    "## 순환 신경망 구조\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png\" width=\"700\">\n",
    "\n",
    "<br>\n",
    "\n",
    "- 입력 $x_t$에서 $t$는 시각을 뜻함\n",
    "\n",
    "- $X_0$에 대한 출력 $Y_0$이 다음 레이어에 전달\n",
    "\n",
    "- 각각의 입력에 대해 출력은 해당 레이어대로 출력값을 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kowOGfSLqbSn"
   },
   "source": [
    "## 순환 신경망의 다양한 구조\n",
    "\n",
    "<img src=\"https://static.packt-cdn.com/products/9781789346640/graphics/2d4a64ef-9cf9-4b4a-9049-cb9de7a07f89.png\">\n",
    "  \n",
    "  <sub>출처: https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781789346640/11/ch11lvl1sec80/introduction</sub>\n",
    "\n",
    "- one to one\n",
    "  - RNN\n",
    "\n",
    "- one to many\n",
    "  - Image Captioning \n",
    "\n",
    "  - 이미지에 대한 설명 생성\n",
    "\n",
    "- many to one\n",
    "  - Sentiment Classification\n",
    "\n",
    "  - 문장의 긍정/부정을 판단하는 감정 분석\n",
    "\n",
    "- many to many\n",
    "  - Machine Translation\n",
    "\n",
    "  - 하나의 언어를 다른 언어로 번역하는 기계 번역\n",
    "\n",
    "- many to many\n",
    "  - Video Classification(Frame Level)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uiIA4Ue7-G5B"
   },
   "source": [
    "## 두 가지 정보(현재 입력, 이전 시각의 출력)을 처리하는 수식\n",
    "$\\qquad h_t = tanh ( \\ h_{t-1} W_h \\ + \\ x_t W_x + b) $\n",
    "\n",
    "- $W_x$ : 입력 $x$를 출력 $h$로 변환하기 위한 가중치\n",
    "\n",
    "- $W_h$ : 다음 시각의 출력으로 변환하기 위한 가중치\n",
    "\n",
    "- $h$는 '상태'를 기억\n",
    "\n",
    "- $h_t \\ $를 은닉 상태(hidden state) 또는 은닉 상태 벡터(hidden state vector)라고도 불림\n",
    "\n",
    "  <sub>출처: https://colah.github.io/posts/2015-08-Understanding-LSTMs/</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lp3_sVIjHk0F"
   },
   "source": [
    "## 순환 신경망 레이어 (RNN Layer)\n",
    "\n",
    "- 입력: `(timesteps, input_features)`\n",
    "\n",
    "- 출력: `(timesteps, output_features)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n08yr0aAIbFD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZKIInEEZIcBj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEZ8dQEEKxlQ"
   },
   "source": [
    "## 케라스의 순환층\n",
    "- `SimpleRNN` layer\n",
    "\n",
    "- 입력: `(batch_size, timesteps, input_features)`\n",
    "\n",
    "- 출력\n",
    "  - `return_sequences`로 결정할 수 있음\n",
    "  \n",
    "  - 3D 텐서\n",
    "    - 타임스텝의 출력을 모은 전체 시퀀스를 반환\n",
    "\n",
    "    - `(batch_size, timesteps, output_features)`\n",
    "\n",
    "  - 2D 텐서\n",
    "    - 입력 시퀀스에 대한 마지막 출력만 반환\n",
    "\n",
    "    - `(batch_size, output_features)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZ1YCi1iKMC8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQ9LbTgoKxIl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KDxE5Jv0Kc7O"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-MDIz1zcIyP"
   },
   "source": [
    "- 네트워크의 표현력을 증가시키기 위해 여러 개의 순환층을 차례대로 쌓는 것이 유용할 때가 있음\n",
    "\n",
    "  - 이런 설정에서는 중간층들이 전체 출력 시퀀스를 반환하도록 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E1tUaIe2L7GZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0CNAzN_deXE"
   },
   "source": [
    "## IMDB 데이터 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1SbNYRdmVTY"
   },
   "source": [
    "### 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-27Tkihbcei2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KL-_piPIdms6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uf2EVnzceS4R"
   },
   "source": [
    "### 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7dfeDC5eMdj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KnAUijOReWGu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pU_9-Hu7mcMa"
   },
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XdPtzPwNeonA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afIyKdLsmj6N"
   },
   "source": [
    "### 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sendhEujeu8S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rjMFpiBe4Fa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLO526Sgf-U_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTmZ_XiZguM5"
   },
   "source": [
    "- 전체 시퀀스가 아니라 순서대로 500개의 단어만 입력했기 때문에 성능이 낮게 나옴\n",
    "\n",
    "- SimpleRNN은 긴 시퀀스를 처리하는데 적합하지 않음\n",
    "\n",
    "- SimpleRNN은 실전에 사용하기엔 너무 단순\n",
    "\n",
    "- SimpleRNN은 이론적으로 시간 $t$ 에서 이전의 모든 타임스텝의 정보를 유지할 수 있지만, 실제로는 긴 시간에 걸친 의존성은 학습할 수 없음\n",
    "\n",
    "- 그래디언트 소실 문제(vanishing gradient problem)\n",
    "  - 이를 방지하기 위해 LSTM, GRU 같은 레이어 등장\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZXJIdDai7sV"
   },
   "source": [
    "# LSTM(Long Short-Term Memory)\n",
    "- 장단기 메모리 알고리즘\n",
    "\n",
    "- 나중을 위해 정보를 저장함으로써 오래된 시그널이 점차 소실되는 것을 막아줌\n",
    "\n",
    "  <img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\">\n",
    "\n",
    "  <sub>출처: https://colah.github.io/posts/2015-08-Understanding-LSTMs/</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3afhQOUlqG9"
   },
   "source": [
    "## IMDB 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZOCK6EQl1QI"
   },
   "source": [
    "### 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00ZS9v3YmT_H"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UW-VG35_gwmt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uo8uhFyBl2k-"
   },
   "source": [
    "### 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFkAN2Dzl3GC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPr9Ec-El50W"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAPnX6ACmenz"
   },
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGWhqX5GmNeR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFiE5SpPmh_x"
   },
   "source": [
    "### 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwQ6ZYHTyXHS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NiZ6uiUPyXHN"
   },
   "source": [
    "### 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZjW6iOV3k-J"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dc0wuvArGHgD"
   },
   "source": [
    "# GRU (Gated Recurrent Unit)\n",
    "- LSTM을 더 단순하게 만든 구조\n",
    "\n",
    "- 기억 셀은 없고, 시간방향으로 전파하는 것은 은닉 상태만 있음\n",
    "\n",
    "- reset gate\n",
    "  - 과거의 은닉 상태를 얼마나 무시할지 결정\n",
    "\n",
    "  - $r$ 값이 결정\n",
    "\n",
    "- update gate\n",
    "  -  은닉 상태를 갱신하는 게이트  \n",
    "\n",
    "  - LSTM의 forget, input gate 역할을 동시에 함\n",
    "  \n",
    "  <img src=\"https://miro.medium.com/max/1400/1*jhi5uOm9PvZfmxvfaCektw.png\" width=\"500\">\n",
    "\n",
    "<sub>출처: https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21</sub>\n",
    "\n",
    "  ### $\\qquad z = \\sigma (x_t W^{(z)}_x + h_{t-1} W^{(z)}_h + b^{(z)} \\\\ \n",
    "  \\qquad r = \\sigma (x_t W^{(r)}_x + h_{t-1} W^{(r)}_h + b^{(r)}) \\\\\n",
    "  \\qquad \\tilde{i} = tanh (x_t W^{(i)}_x + (r \\odot h_{t-1}) W^{(i)}_h + b ) \\\\\n",
    "  \\qquad h_t = (1 - z) \\odot h_{t-1} + z \\odot \\tilde{h}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TvnFLeqSsOSh"
   },
   "source": [
    "## Reuters 데이터\n",
    "\n",
    "- IMDB와 유사한 데이터셋(텍스트 데이터)\n",
    "\n",
    "- 46개의 상호 배타적인 토픽으로 이루어진 데이터셋 \n",
    "  - 다중 분류 문제\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONPaMDFEsftq"
   },
   "source": [
    "### 데이터셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sFyvFVizsUVD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C234q3AVsUSp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VioYGD8Ms4zH"
   },
   "source": [
    "### 데이터 전처리 및 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Sb0OxNlsUQT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rNUkve6-sUN6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBxQZtOJsULl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6hwQtn_sUJN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_P_JxBFntQOP"
   },
   "source": [
    "### 모델 구성\n",
    "- LSTM 레이어도 SimpleRNN과 같이 `return_sequences` 인자 사용가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1vG3ZGSksUG5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K_uMdxmysUEg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjgupW2puHXA"
   },
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T22YsRQ8sUBg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGZhIiYeuSCD"
   },
   "source": [
    "### 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "INJu0-itsT_T"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLZLATDBuTtj"
   },
   "source": [
    "### 모델 평가\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVxVV-kmsT6D"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "_8 순환 신경망(Recurrent Neural Network).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
